{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ec9f3054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (1.0.8)\n",
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (5.1.2)\n",
      "Requirement already satisfied: faiss-cpu in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (1.13.0)\n",
      "Requirement already satisfied: streamlit in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (1.51.0)\n",
      "Requirement already satisfied: python-docx in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (1.2.0)\n",
      "Requirement already satisfied: pdfplumber in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (0.11.8)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (4.14.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.6 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain) (1.0.7)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain) (1.0.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain) (2.12.4)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (4.57.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (1.6.0)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (12.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from faiss-cpu) (2.2.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<7,>=4.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (6.2.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (2.2.3)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (6.33.1)\n",
      "Requirement already satisfied: pyarrow<22,>=7.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (21.0.0)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (3.1.45)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from python-docx) (6.0.2)\n",
      "Requirement already satisfied: pdfminer.six==20251107 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pdfplumber) (20251107)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pdfplumber) (5.0.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pdfminer.six==20251107->pdfplumber) (3.4.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pdfminer.six==20251107->pdfplumber) (46.0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.12.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (0.4.44)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.4)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.9)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: cffi>=2.0.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.0.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.6->langchain) (3.0.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.29.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (0.25.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.23)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain sentence-transformers faiss-cpu streamlit python-docx pdfplumber beautifulsoup4 requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "37e2881f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup successful!\n",
      "Embedding shape: (1, 384)\n",
      "Index size: 1\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "print(\"Setup successful!\")\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vec = model.encode([\"hello world\"])\n",
    "print(\"Embedding shape:\", vec.shape)\n",
    "\n",
    "# FAISS test\n",
    "dim = vec.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "faiss.normalize_L2(vec)\n",
    "index.add(vec)\n",
    "print(\"Index size:\", index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d549d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec  = model.encode[\"ssd\"]\n",
    "dim = vec.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "faiss.normalize(vec)\n",
    "index.add(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "41b1d70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders created: ['data_raw', 'data_processed']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folders = [\"data_raw\", \"data_processed\"]\n",
    "for f in folders:\n",
    "    os.makedirs(f, exist_ok=True)\n",
    "\n",
    "print(\"Folders created:\", folders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d6c9ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def load_pdf(path):\n",
    "    pages_text = []\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text() or \"\"\n",
    "            pages_text.append(text)\n",
    "    return pages_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fea0dab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Somalia Flood Exposure Methodology Note\\nAnalysis for 2024 HNRP\\nThis technical note summarises the methodology used to calculate the number of people\\npotentially exposed to flooding in Somalia in the 2024 Somalia Humanitarian Needs and\\nResponse Plan (HNRP). The UN OCHA Centre for Humanitarian Data worked with awiderange\\nof technical partners to develop a methodology that was then endorsed by the Somalia ICCG\\nandHCT.\\nDaily FloodScan (1998-2022) & WorldPop (2020 UN Adjusted) raster data wasanalysedtogain\\nunderstanding of flood conditions across Somalia for both March-April-May (MAM) and\\nOctober-November-December (OND) seasons. FloodScan daily flood fraction Standard Flood\\nExposure Depiction (SFED) was aggregated to yearly seasonal maximum fraction composites\\nforboththeMAMandONDseasonsforallyearsofhistoricalFloodScandata(1998-2022).\\nTheyearlyseasonalSFEDcompositeswerethenprocessed/reclassifiedintwodistinctways:\\n1. Compositesreclassifiedtobinaryusinga20percentfloodfractionthreshold.\\n2. Composites masked to just remove any flood fraction values < 0.05 percent and the\\nfloodfraction (0-100percent)retained.\\nAll composites in both sets of processed yearly-seasonal flood fraction rasters were then\\nmultiplied by the WorldPop (2020 UNAdjusted)populationestimaterastertocreatetwosetsof\\nyearly seasonal flood exposure rasters. These were then aggregated at the second\\nadministrative level via zonal statistics (sum) to obtain the estimated population exposure per\\ndistrict for each season across all years for each set. These estimates were converted to the\\npercent of population exposed by dividing the estimates by the total population per district\\n(from World Pop raster). The percent exposure figure was then applied to the updated 2024\\npopulation data set (UN OCHA, UNFPA Methodology) to obtain updated population exposure\\nestimates.Thesedatawerefurtheraggregatedtoadministrativelevel1.\\nRanges for exposure were estimated using percentiles for boththeMAMandONDseasonsper\\nadministrative level. As ECMWF seasonal forecast predicts an above average precipitation\\nseason for MAM 2024, the range of populationexposed wasbasedonthe50th-95thpercentile\\nlevels. Since there was no available data to inform predictions for OND 2024, the 25-75th\\npercentile values were used to bound the range. The lower and upper limits of therangeswere\\nPage1of2']\n"
     ]
    }
   ],
   "source": [
    "pages = load_pdf('data_raw/sample.pdf')\n",
    "print(pages[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3e3a83",
   "metadata": {},
   "source": [
    "# pull text from any website URL so it can use that information for retrieval & answering questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ee7ac0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Domain\n",
      "Example Domain\n",
      "This domain is for use in documentation examples without needing permission. Avoid use in operations.\n",
      "Learn more\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def load_url(url):\n",
    "    response = requests.get(url, timeout=10)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # remove script + style\n",
    "    for tag in soup([\"script\", \"style\"]):\n",
    "        tag.decompose()\n",
    "    \n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "    clean_text = \"\\n\".join([line.strip() for line in text.splitlines() if line.strip()])\n",
    "    return clean_text\n",
    "\n",
    "# Example:\n",
    "text = load_url(\"https://example.com\")\n",
    "print(text[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400ab32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_file(path_or_url):\n",
    "    if path_or_url.startswith(\"http\"):\n",
    "        content = load_url(path_or_url)\n",
    "        return [{\"source\": path_or_url, \"text\": content}]\n",
    "    \n",
    "    if path_or_url.endswith(\".pdf\"):\n",
    "        pages = load_pdf(path_or_url)\n",
    "        return [{\"source\": path_or_url, \"text\": p} for p in pages]\n",
    "    \n",
    "\"\"\"     if path_or_url.endswith(\".csv\"):\n",
    "        rows = load_csv(path_or_url)\n",
    "        return [{\"source\": path_or_url, \"text\": r} for r in rows] \"\"\"\n",
    "    \n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "de123f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 2 items\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'source': 'data_raw/sample.pdf',\n",
       "  'text': 'Somalia Flood Exposure Methodology Note\\nAnalysis for 2024 HNRP\\nThis technical note summarises the methodology used to calculate the number of people\\npotentially exposed to flooding in Somalia in the 2024 Somalia Humanitarian Needs and\\nResponse Plan (HNRP). The UN OCHA Centre for Humanitarian Data worked with awiderange\\nof technical partners to develop a methodology that was then endorsed by the Somalia ICCG\\nandHCT.\\nDaily FloodScan (1998-2022) & WorldPop (2020 UN Adjusted) raster data wasanalysedtogain\\nunderstanding of flood conditions across Somalia for both March-April-May (MAM) and\\nOctober-November-December (OND) seasons. FloodScan daily flood fraction Standard Flood\\nExposure Depiction (SFED) was aggregated to yearly seasonal maximum fraction composites\\nforboththeMAMandONDseasonsforallyearsofhistoricalFloodScandata(1998-2022).\\nTheyearlyseasonalSFEDcompositeswerethenprocessed/reclassifiedintwodistinctways:\\n1. Compositesreclassifiedtobinaryusinga20percentfloodfractionthreshold.\\n2. Composites masked to just remove any flood fraction values < 0.05 percent and the\\nfloodfraction (0-100percent)retained.\\nAll composites in both sets of processed yearly-seasonal flood fraction rasters were then\\nmultiplied by the WorldPop (2020 UNAdjusted)populationestimaterastertocreatetwosetsof\\nyearly seasonal flood exposure rasters. These were then aggregated at the second\\nadministrative level via zonal statistics (sum) to obtain the estimated population exposure per\\ndistrict for each season across all years for each set. These estimates were converted to the\\npercent of population exposed by dividing the estimates by the total population per district\\n(from World Pop raster). The percent exposure figure was then applied to the updated 2024\\npopulation data set (UN OCHA, UNFPA Methodology) to obtain updated population exposure\\nestimates.Thesedatawerefurtheraggregatedtoadministrativelevel1.\\nRanges for exposure were estimated using percentiles for boththeMAMandONDseasonsper\\nadministrative level. As ECMWF seasonal forecast predicts an above average precipitation\\nseason for MAM 2024, the range of populationexposed wasbasedonthe50th-95thpercentile\\nlevels. Since there was no available data to inform predictions for OND 2024, the 25-75th\\npercentile values were used to bound the range. The lower and upper limits of therangeswere\\nPage1of2'},\n",
       " {'source': 'data_raw/sample.pdf',\n",
       "  'text': 'calculated at the administrative level using all of the historical yearly seasonal flood exposure\\nestimates. The two sets of range estimates were combined conservatively by taking the\\nmaximumvalueforboththeupperandlowerboundsforeachseason.\\nContact the OCHA Centre for Humanitarian Data via Leonardo Milano, Team Lead for Data\\nScienceatleonardo.milano@un.orgwithanyquestionsorfeedback.\\nPage2of2'}]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pdf = \"data_raw/sample.pdf\"\n",
    "docs = ingest_file(test_pdf)\n",
    "\n",
    "print(\"Loaded:\", len(docs), \"items\")\n",
    "docs[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d58a914",
   "metadata": {},
   "source": [
    "# Processing & Chunking Text\n",
    "Chunking is critical for RAG.\n",
    "Instead of sending entire documents to the model, we break them into small meaningful pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "588b5720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (1.0.8)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.6 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain) (1.0.7)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain) (1.0.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain) (2.12.4)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (0.4.44)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (24.2)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (4.15.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.4)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.9)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.6->langchain) (3.0.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (2.32.3)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (0.25.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (4.11.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (2.2.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-text-splitters in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (1.0.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-text-splitters) (1.0.7)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.4.44)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.12.4)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.32.3)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.4.2)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (4.11.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.2.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain-text-splitters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "75846c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b2b77629",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=150,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "def chunk_documents(docs):\n",
    "    \"\"\"\n",
    "    Input: docs = [{\"source\": \"...\", \"text\": \"...\"}]\n",
    "    Output: list of chunks with metadata\n",
    "    \"\"\"\n",
    "    processed = []\n",
    "\n",
    "    for d in docs:\n",
    "        chunks = text_splitter.split_text(d[\"text\"])\n",
    "        for i, c in enumerate(chunks):\n",
    "            processed.append({\n",
    "                \"source\": d[\"source\"],\n",
    "                \"chunk_id\": i,\n",
    "                \"text\": c\n",
    "            })\n",
    "\n",
    "    return processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "eccbdda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original pages: 2\n",
      "Total chunks: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'source': 'data_raw/sample.pdf',\n",
       "  'chunk_id': 0,\n",
       "  'text': 'Somalia Flood Exposure Methodology Note\\nAnalysis for 2024 HNRP\\nThis technical note summarises the methodology used to calculate the number of people\\npotentially exposed to flooding in Somalia in the 2024 Somalia Humanitarian Needs and\\nResponse Plan (HNRP). The UN OCHA Centre for Humanitarian Data worked with awiderange\\nof technical partners to develop a methodology that was then endorsed by the Somalia ICCG\\nandHCT.\\nDaily FloodScan (1998-2022) & WorldPop (2020 UN Adjusted) raster data wasanalysedtogain\\nunderstanding of flood conditions across Somalia for both March-April-May (MAM) and\\nOctober-November-December (OND) seasons. FloodScan daily flood fraction Standard Flood\\nExposure Depiction (SFED) was aggregated to yearly seasonal maximum fraction composites'},\n",
       " {'source': 'data_raw/sample.pdf',\n",
       "  'chunk_id': 1,\n",
       "  'text': 'Exposure Depiction (SFED) was aggregated to yearly seasonal maximum fraction composites\\nforboththeMAMandONDseasonsforallyearsofhistoricalFloodScandata(1998-2022).\\nTheyearlyseasonalSFEDcompositeswerethenprocessed/reclassifiedintwodistinctways:\\n1. Compositesreclassifiedtobinaryusinga20percentfloodfractionthreshold.\\n2. Composites masked to just remove any flood fraction values < 0.05 percent and the\\nfloodfraction (0-100percent)retained.\\nAll composites in both sets of processed yearly-seasonal flood fraction rasters were then\\nmultiplied by the WorldPop (2020 UNAdjusted)populationestimaterastertocreatetwosetsof\\nyearly seasonal flood exposure rasters. These were then aggregated at the second\\nadministrative level via zonal statistics (sum) to obtain the estimated population exposure per'}]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = ingest_file(\"data_raw/sample.pdf\")  # reuse from Step 2\n",
    "chunks = chunk_documents(docs)\n",
    "\n",
    "print(\"Original pages:\", len(docs))\n",
    "print(\"Total chunks:\", len(chunks))\n",
    "chunks[:2]  # show first two chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "04fcbdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_chunks(chunks, filename=\"processed.json\"):\n",
    "    path = f\"data_processed/{filename}\"\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(chunks, f, indent=2)\n",
    "    print(\"Saved:\", path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a57b78b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data_processed/sample_chunks.json\n"
     ]
    }
   ],
   "source": [
    "save_chunks(chunks, \"sample_chunks.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c31ff5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chunks(filename=\"processed.json\"):\n",
    "    with open(f\"data_processed/{filename}\", \"r\") as f:\n",
    "        return json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546f7c08",
   "metadata": {},
   "source": [
    "# Embeddings + Vector Database (FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f39e2202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded chunks: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'source': 'data_raw/sample.pdf',\n",
       "  'chunk_id': 0,\n",
       "  'text': 'Somalia Flood Exposure Methodology Note\\nAnalysis for 2024 HNRP\\nThis technical note summarises the methodology used to calculate the number of people\\npotentially exposed to flooding in Somalia in the 2024 Somalia Humanitarian Needs and\\nResponse Plan (HNRP). The UN OCHA Centre for Humanitarian Data worked with awiderange\\nof technical partners to develop a methodology that was then endorsed by the Somalia ICCG\\nandHCT.\\nDaily FloodScan (1998-2022) & WorldPop (2020 UN Adjusted) raster data wasanalysedtogain\\nunderstanding of flood conditions across Somalia for both March-April-May (MAM) and\\nOctober-November-December (OND) seasons. FloodScan daily flood fraction Standard Flood\\nExposure Depiction (SFED) was aggregated to yearly seasonal maximum fraction composites'},\n",
       " {'source': 'data_raw/sample.pdf',\n",
       "  'chunk_id': 1,\n",
       "  'text': 'Exposure Depiction (SFED) was aggregated to yearly seasonal maximum fraction composites\\nforboththeMAMandONDseasonsforallyearsofhistoricalFloodScandata(1998-2022).\\nTheyearlyseasonalSFEDcompositeswerethenprocessed/reclassifiedintwodistinctways:\\n1. Compositesreclassifiedtobinaryusinga20percentfloodfractionthreshold.\\n2. Composites masked to just remove any flood fraction values < 0.05 percent and the\\nfloodfraction (0-100percent)retained.\\nAll composites in both sets of processed yearly-seasonal flood fraction rasters were then\\nmultiplied by the WorldPop (2020 UNAdjusted)populationestimaterastertocreatetwosetsof\\nyearly seasonal flood exposure rasters. These were then aggregated at the second\\nadministrative level via zonal statistics (sum) to obtain the estimated population exposure per'}]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_chunks(filename):\n",
    "    with open(f\"data_processed/{filename}\", \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "chunks = load_chunks(\"sample_chunks.json\")\n",
    "print(\"Loaded chunks:\", len(chunks))\n",
    "chunks[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3cd24c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded!\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"Embedding model loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e3dc3ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5, 384)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def embed_chunks(chunks):\n",
    "    texts = [c[\"text\"] for c in chunks]\n",
    "    embeddings = embed_model.encode(\n",
    "        texts, \n",
    "        convert_to_numpy=True, \n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "embeddings = embed_chunks(chunks)\n",
    "embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "64724339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index size: 5\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "def build_faiss_index(embeddings):\n",
    "    dim = embeddings.shape[1]\n",
    "\n",
    "    # Create IndexFlatIP (Inner Product = cosine similarity after normalization)\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "\n",
    "    # Normalize to make inner product = cosine similarity\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    \n",
    "    # Add vectors to index\n",
    "    index.add(embeddings)\n",
    "\n",
    "    return index\n",
    "\n",
    "index = build_faiss_index(embeddings)\n",
    "print(\"FAISS index size:\", index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5ce4c5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: faiss_store.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def save_faiss(index, chunks, embeddings, filename=\"faiss_store.pkl\"):\n",
    "    store = {\n",
    "        \"index\": index,\n",
    "        \"chunks\": chunks,\n",
    "        \"embeddings\": embeddings\n",
    "    }\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(store, f)\n",
    "    print(\"Saved:\", filename)\n",
    "\n",
    "save_faiss(index, chunks, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1a83fd37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['index', 'chunks', 'embeddings'])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_faiss(filename=\"faiss_store.pkl\"):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "faiss_store = load_faiss()\n",
    "faiss_store.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "68794f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "Score: 0.13010777533054352\n",
      "Source: data_raw/sample.pdf\n",
      "Chunk: 0\n",
      "Text Preview: Somalia Flood Exposure Methodology Note\n",
      "Analysis for 2024 HNRP\n",
      "This technical note summarises the methodology used to calculate the number of people\n",
      "potentially exposed to flooding in Somalia in the 2024 Somalia Humanitarian Needs and\n",
      "Response Plan (HNRP). The UN OCHA Centre for Humanitarian Data wo ...\n",
      "-----\n",
      "Score: 0.11402948200702667\n",
      "Source: data_raw/sample.pdf\n",
      "Chunk: 3\n",
      "Text Preview: season for MAM 2024, the range of populationexposed wasbasedonthe50th-95thpercentile\n",
      "levels. Since there was no available data to inform predictions for OND 2024, the 25-75th\n",
      "percentile values were used to bound the range. The lower and upper limits of therangeswere\n",
      "Page1of2 ...\n",
      "-----\n",
      "Score: 0.09291001409292221\n",
      "Source: data_raw/sample.pdf\n",
      "Chunk: 0\n",
      "Text Preview: calculated at the administrative level using all of the historical yearly seasonal flood exposure\n",
      "estimates. The two sets of range estimates were combined conservatively by taking the\n",
      "maximumvalueforboththeupperandlowerboundsforeachseason.\n",
      "Contact the OCHA Centre for Humanitarian Data via Leonardo M ...\n"
     ]
    }
   ],
   "source": [
    "def search_index(query, store, k=3):\n",
    "    index = store[\"index\"]\n",
    "    chunks = store[\"chunks\"]\n",
    "\n",
    "    q_emb = embed_model.encode([query], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(q_emb)\n",
    "\n",
    "    scores, indices = index.search(q_emb, k)\n",
    "\n",
    "    results = []\n",
    "    for idx, score in zip(indices[0], scores[0]):\n",
    "        results.append({\n",
    "            \"score\": float(score),\n",
    "            \"source\": chunks[idx][\"source\"],\n",
    "            \"chunk_id\": chunks[idx][\"chunk_id\"],\n",
    "            \"text\": chunks[idx][\"text\"]\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "query = \"What is the document talking about?\"\n",
    "results = search_index(query, faiss_store, k=3)\n",
    "\n",
    "for r in results:\n",
    "    print(\"-----\")\n",
    "    print(\"Score:\", r[\"score\"])\n",
    "    print(\"Source:\", r[\"source\"])\n",
    "    print(\"Chunk:\", r[\"chunk_id\"])\n",
    "    print(\"Text Preview:\", r[\"text\"][:300], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87a50f5",
   "metadata": {},
   "source": [
    "# Reranking + LLM Answer Generation + Citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "59da7574",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (5.1.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (4.57.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (1.6.0)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (12.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers\n",
    "!pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2bb2ceda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranker loaded!\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "print(\"Reranker loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ac509ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_results(query, retrieved_chunks):\n",
    "    pairs = [[query, item[\"text\"]] for item in retrieved_chunks]\n",
    "    scores = reranker.predict(pairs)\n",
    "\n",
    "    for item, score in zip(retrieved_chunks, scores):\n",
    "        item[\"rerank_score\"] = float(score)\n",
    "\n",
    "    # Sort by score (descending)\n",
    "    ranked = sorted(retrieved_chunks, key=lambda x: x[\"rerank_score\"], reverse=True)\n",
    "\n",
    "    return ranked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "751ed47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, top_chunks):\n",
    "    context = \"\"\n",
    "\n",
    "    for c in top_chunks:\n",
    "        context += f\"\\n[Source: {c['source']} | Chunk: {c['chunk_id']}] \\n{c['text']}\\n\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant. Use ONLY the information from the context below to answer the user's question.\n",
    "\n",
    "If the context does not contain the answer, reply:\n",
    "\"I don’t know based on the provided documents.\"\n",
    "\n",
    "Always include a \"Sources:\" section at the end.\n",
    "\n",
    "### CONTEXT:\n",
    "{context}\n",
    "\n",
    "### QUESTION:\n",
    "{query}\n",
    "\n",
    "### ANSWER (with citations):\n",
    "\"\"\"\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332e7b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROQ KEY = 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\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "from groq import Groq\n",
    "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "40811d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/apoorvnathtripathi/Desktop/multi_source_rag_project\n",
      "Environment key value: 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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Environment key value:\", os.getenv(\"GROQ_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72e3882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<groq.Groq at 0x123856b70>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "667a2105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR TYPE: <class 'groq.BadRequestError'>\n",
      "ERROR DETAILS: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    test = client.chat.completions.create(\n",
    "        model=\"llama-3.1-70b-versatile\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Say hello\"}],\n",
    "        max_tokens=20\n",
    "    )\n",
    "    print(test.choices[0].message.content)\n",
    "\n",
    "except Exception as e:\n",
    "    import json\n",
    "    print(\"ERROR TYPE:\", type(e))\n",
    "    try:\n",
    "        print(\"ERROR DETAILS:\", e.args[0])\n",
    "    except:\n",
    "        print(\"RAW ERROR:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "533f7893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_groq(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ac44c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline_groq(query, store, k=5):\n",
    "    # 1. Retrieve from FAISS\n",
    "    retrieved = search_index(query, store, k)\n",
    "\n",
    "    # 2. Rerank retrieved chunks\n",
    "    ranked = rerank_results(query, retrieved)\n",
    "\n",
    "    # 3. Pick top 3\n",
    "    top_chunks = ranked[:3]\n",
    "\n",
    "    # 4. Build the LLM prompt\n",
    "    prompt = build_prompt(query, top_chunks)\n",
    "    \n",
    "    # 5. Get the answer from Groq LLaMA 3.1\n",
    "    answer = generate_answer_groq(prompt)\n",
    "\n",
    "    return answer, top_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "64bc8406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/llama-guard-4-12b\n",
      "llama-3.1-8b-instant\n",
      "groq/compound\n",
      "playai-tts-arabic\n",
      "openai/gpt-oss-20b\n",
      "whisper-large-v3\n",
      "openai/gpt-oss-safeguard-20b\n",
      "playai-tts\n",
      "meta-llama/llama-4-maverick-17b-128e-instruct\n",
      "groq/compound-mini\n",
      "whisper-large-v3-turbo\n",
      "meta-llama/llama-4-scout-17b-16e-instruct\n",
      "moonshotai/kimi-k2-instruct-0905\n",
      "qwen/qwen3-32b\n",
      "moonshotai/kimi-k2-instruct\n",
      "llama-3.3-70b-versatile\n",
      "openai/gpt-oss-120b\n",
      "meta-llama/llama-prompt-guard-2-22m\n",
      "allam-2-7b\n",
      "meta-llama/llama-prompt-guard-2-86m\n"
     ]
    }
   ],
   "source": [
    "models = client.models.list()\n",
    "for m in models.data:\n",
    "    print(m.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e91b4dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:\n",
      " The document is discussing the methodology used to calculate the number of people potentially exposed to flooding in Somalia, specifically for the 2024 Somalia Humanitarian Needs and Response Plan (HNRP). It outlines the process of aggregating and analyzing flood data, including the use of Daily FloodScan and WorldPop raster data, to estimate population exposure to flooding.\n",
      "\n",
      "Sources:\n",
      "[Source: data_raw/sample.pdf | Chunk: 1], \n",
      "[Source: data_raw/sample.pdf | Chunk: 0]\n",
      "\n",
      "--- SOURCES USED ---\n",
      "data_raw/sample.pdf (chunk 1)\n",
      "data_raw/sample.pdf (chunk 0)\n",
      "data_raw/sample.pdf (chunk 0)\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the document talking about?\"\n",
    "\n",
    "answer, used_chunks = rag_pipeline_groq(query, faiss_store)\n",
    "\n",
    "\n",
    "print(\"ANSWER:\\n\", answer)\n",
    "print(\"\\n--- SOURCES USED ---\")\n",
    "for c in used_chunks:\n",
    "    print(f\"{c['source']} (chunk {c['chunk_id']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "62a1ccc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (1.51.0)\n",
      "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<7,>=4.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (6.2.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (2.2.0)\n",
      "Requirement already satisfied: packaging<26,>=20 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (2.2.3)\n",
      "Requirement already satisfied: pillow<13,>=7.1.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (12.0.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (6.33.1)\n",
      "Requirement already satisfied: pyarrow<22,>=7.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (21.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (4.15.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (3.1.45)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.12.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.29.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install streamlit\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
