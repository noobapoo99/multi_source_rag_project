{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ec9f3054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (1.0.8)\n",
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (5.1.2)\n",
      "Requirement already satisfied: faiss-cpu in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (1.13.0)\n",
      "Requirement already satisfied: streamlit in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (1.51.0)\n",
      "Requirement already satisfied: python-docx in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (1.2.0)\n",
      "Requirement already satisfied: pdfplumber in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (0.11.8)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (4.14.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.6 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain) (1.0.7)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain) (1.0.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain) (2.12.4)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (4.57.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (1.6.0)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (12.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from faiss-cpu) (2.2.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<7,>=4.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (6.2.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (2.2.3)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (6.33.1)\n",
      "Requirement already satisfied: pyarrow<22,>=7.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (21.0.0)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (3.1.45)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from python-docx) (6.0.2)\n",
      "Requirement already satisfied: pdfminer.six==20251107 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pdfplumber) (20251107)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pdfplumber) (5.0.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pdfminer.six==20251107->pdfplumber) (3.4.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pdfminer.six==20251107->pdfplumber) (46.0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.12.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (0.4.44)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.4)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.9)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: cffi>=2.0.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.0.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.6->langchain) (3.0.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.29.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (0.25.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.23)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain sentence-transformers faiss-cpu streamlit python-docx pdfplumber beautifulsoup4 requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e2881f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup successful!\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "print(\"Setup successful!\")\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vec = model.encode([\"hello world\"])\n",
    "print(\"Embedding shape:\", vec.shape)\n",
    "\n",
    "# FAISS test\n",
    "dim = vec.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "faiss.normalize_L2(vec)\n",
    "index.add(vec)\n",
    "print(\"Index size:\", index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b1d70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders created: ['data_raw', 'data_processed']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folders = [\"data_raw\", \"data_processed\"]\n",
    "for f in folders:\n",
    "    os.makedirs(f, exist_ok=True)\n",
    "\n",
    "print(\"Folders created:\", folders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c9ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def load_pdf(path):\n",
    "    pages_text = []\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text() or \"\"\n",
    "            pages_text.append(text)\n",
    "    return pages_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea0dab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Somalia Flood Exposure Methodology Note\\nAnalysis for 2024 HNRP\\nThis technical note summarises the methodology used to calculate the number of people\\npotentially exposed to flooding in Somalia in the 2024 Somalia Humanitarian Needs and\\nResponse Plan (HNRP). The UN OCHA Centre for Humanitarian Data worked with awiderange\\nof technical partners to develop a methodology that was then endorsed by the Somalia ICCG\\nandHCT.\\nDaily FloodScan (1998-2022) & WorldPop (2020 UN Adjusted) raster data wasanalysedtogain\\nunderstanding of flood conditions across Somalia for both March-April-May (MAM) and\\nOctober-November-December (OND) seasons. FloodScan daily flood fraction Standard Flood\\nExposure Depiction (SFED) was aggregated to yearly seasonal maximum fraction composites\\nforboththeMAMandONDseasonsforallyearsofhistoricalFloodScandata(1998-2022).\\nTheyearlyseasonalSFEDcompositeswerethenprocessed/reclassifiedintwodistinctways:\\n1. Compositesreclassifiedtobinaryusinga20percentfloodfractionthreshold.\\n2. Composites masked to just remove any flood fraction values < 0.05 percent and the\\nfloodfraction (0-100percent)retained.\\nAll composites in both sets of processed yearly-seasonal flood fraction rasters were then\\nmultiplied by the WorldPop (2020 UNAdjusted)populationestimaterastertocreatetwosetsof\\nyearly seasonal flood exposure rasters. These were then aggregated at the second\\nadministrative level via zonal statistics (sum) to obtain the estimated population exposure per\\ndistrict for each season across all years for each set. These estimates were converted to the\\npercent of population exposed by dividing the estimates by the total population per district\\n(from World Pop raster). The percent exposure figure was then applied to the updated 2024\\npopulation data set (UN OCHA, UNFPA Methodology) to obtain updated population exposure\\nestimates.Thesedatawerefurtheraggregatedtoadministrativelevel1.\\nRanges for exposure were estimated using percentiles for boththeMAMandONDseasonsper\\nadministrative level. As ECMWF seasonal forecast predicts an above average precipitation\\nseason for MAM 2024, the range of populationexposed wasbasedonthe50th-95thpercentile\\nlevels. Since there was no available data to inform predictions for OND 2024, the 25-75th\\npercentile values were used to bound the range. The lower and upper limits of therangeswere\\nPage1of2']\n"
     ]
    }
   ],
   "source": [
    "pages = load_pdf('data_raw/sample.pdf')\n",
    "print(pages[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3e3a83",
   "metadata": {},
   "source": [
    "# pull text from any website URL so it can use that information for retrieval & answering questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7ac0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Domain\n",
      "Example Domain\n",
      "This domain is for use in documentation examples without needing permission. Avoid use in operations.\n",
      "Learn more\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def load_url(url):\n",
    "    response = requests.get(url, timeout=10)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # remove script + style\n",
    "    for tag in soup([\"script\", \"style\"]):\n",
    "        tag.decompose()\n",
    "    \n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "    clean_text = \"\\n\".join([line.strip() for line in text.splitlines() if line.strip()])\n",
    "    return clean_text\n",
    "\n",
    "# Example:\n",
    "text = load_url(\"https://example.com\")\n",
    "print(text[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400ab32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_file(path_or_url):\n",
    "    if path_or_url.startswith(\"http\"):\n",
    "        content = load_url(path_or_url)\n",
    "        return [{\"source\": path_or_url, \"text\": content}]\n",
    "    \n",
    "    if path_or_url.endswith(\".pdf\"):\n",
    "        pages = load_pdf(path_or_url)\n",
    "        return [{\"source\": path_or_url, \"text\": p} for p in pages]\n",
    "    \n",
    "    if path_or_url.endswith(\".csv\"):\n",
    "        rows = load_csv(path_or_url)\n",
    "        return [{\"source\": path_or_url, \"text\": r} for r in rows]\n",
    "    \n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de123f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 2 items\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'source': 'data_raw/sample.pdf',\n",
       "  'text': 'Somalia Flood Exposure Methodology Note\\nAnalysis for 2024 HNRP\\nThis technical note summarises the methodology used to calculate the number of people\\npotentially exposed to flooding in Somalia in the 2024 Somalia Humanitarian Needs and\\nResponse Plan (HNRP). The UN OCHA Centre for Humanitarian Data worked with awiderange\\nof technical partners to develop a methodology that was then endorsed by the Somalia ICCG\\nandHCT.\\nDaily FloodScan (1998-2022) & WorldPop (2020 UN Adjusted) raster data wasanalysedtogain\\nunderstanding of flood conditions across Somalia for both March-April-May (MAM) and\\nOctober-November-December (OND) seasons. FloodScan daily flood fraction Standard Flood\\nExposure Depiction (SFED) was aggregated to yearly seasonal maximum fraction composites\\nforboththeMAMandONDseasonsforallyearsofhistoricalFloodScandata(1998-2022).\\nTheyearlyseasonalSFEDcompositeswerethenprocessed/reclassifiedintwodistinctways:\\n1. Compositesreclassifiedtobinaryusinga20percentfloodfractionthreshold.\\n2. Composites masked to just remove any flood fraction values < 0.05 percent and the\\nfloodfraction (0-100percent)retained.\\nAll composites in both sets of processed yearly-seasonal flood fraction rasters were then\\nmultiplied by the WorldPop (2020 UNAdjusted)populationestimaterastertocreatetwosetsof\\nyearly seasonal flood exposure rasters. These were then aggregated at the second\\nadministrative level via zonal statistics (sum) to obtain the estimated population exposure per\\ndistrict for each season across all years for each set. These estimates were converted to the\\npercent of population exposed by dividing the estimates by the total population per district\\n(from World Pop raster). The percent exposure figure was then applied to the updated 2024\\npopulation data set (UN OCHA, UNFPA Methodology) to obtain updated population exposure\\nestimates.Thesedatawerefurtheraggregatedtoadministrativelevel1.\\nRanges for exposure were estimated using percentiles for boththeMAMandONDseasonsper\\nadministrative level. As ECMWF seasonal forecast predicts an above average precipitation\\nseason for MAM 2024, the range of populationexposed wasbasedonthe50th-95thpercentile\\nlevels. Since there was no available data to inform predictions for OND 2024, the 25-75th\\npercentile values were used to bound the range. The lower and upper limits of therangeswere\\nPage1of2'},\n",
       " {'source': 'data_raw/sample.pdf',\n",
       "  'text': 'calculated at the administrative level using all of the historical yearly seasonal flood exposure\\nestimates. The two sets of range estimates were combined conservatively by taking the\\nmaximumvalueforboththeupperandlowerboundsforeachseason.\\nContact the OCHA Centre for Humanitarian Data via Leonardo Milano, Team Lead for Data\\nScienceatleonardo.milano@un.orgwithanyquestionsorfeedback.\\nPage2of2'}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pdf = \"data_raw/sample.pdf\"\n",
    "docs = ingest_file(test_pdf)\n",
    "\n",
    "print(\"Loaded:\", len(docs), \"items\")\n",
    "docs[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d58a914",
   "metadata": {},
   "source": [
    "# Processing & Chunking Text\n",
    "Chunking is critical for RAG.\n",
    "Instead of sending entire documents to the model, we break them into small meaningful pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588b5720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (1.0.8)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.6 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain) (1.0.7)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain) (1.0.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain) (2.12.4)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (0.4.44)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (24.2)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (4.15.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.4)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.9)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.6->langchain) (3.0.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (2.32.3)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (0.25.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (4.11.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (2.2.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-text-splitters in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (1.0.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-text-splitters) (1.0.7)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.4.44)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.12.4)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.32.3)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.4.2)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (4.11.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.2.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain-text-splitters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75846c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b77629",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=150,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "def chunk_documents(docs):\n",
    "    \"\"\"\n",
    "    Input: docs = [{\"source\": \"...\", \"text\": \"...\"}]\n",
    "    Output: list of chunks with metadata\n",
    "    \"\"\"\n",
    "    processed = []\n",
    "\n",
    "    for d in docs:\n",
    "        chunks = text_splitter.split_text(d[\"text\"])\n",
    "        for i, c in enumerate(chunks):\n",
    "            processed.append({\n",
    "                \"source\": d[\"source\"],\n",
    "                \"chunk_id\": i,\n",
    "                \"text\": c\n",
    "            })\n",
    "\n",
    "    return processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccbdda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original pages: 2\n",
      "Total chunks: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'source': 'data_raw/sample.pdf',\n",
       "  'chunk_id': 0,\n",
       "  'text': 'Somalia Flood Exposure Methodology Note\\nAnalysis for 2024 HNRP\\nThis technical note summarises the methodology used to calculate the number of people\\npotentially exposed to flooding in Somalia in the 2024 Somalia Humanitarian Needs and\\nResponse Plan (HNRP). The UN OCHA Centre for Humanitarian Data worked with awiderange\\nof technical partners to develop a methodology that was then endorsed by the Somalia ICCG\\nandHCT.\\nDaily FloodScan (1998-2022) & WorldPop (2020 UN Adjusted) raster data wasanalysedtogain\\nunderstanding of flood conditions across Somalia for both March-April-May (MAM) and\\nOctober-November-December (OND) seasons. FloodScan daily flood fraction Standard Flood\\nExposure Depiction (SFED) was aggregated to yearly seasonal maximum fraction composites'},\n",
       " {'source': 'data_raw/sample.pdf',\n",
       "  'chunk_id': 1,\n",
       "  'text': 'Exposure Depiction (SFED) was aggregated to yearly seasonal maximum fraction composites\\nforboththeMAMandONDseasonsforallyearsofhistoricalFloodScandata(1998-2022).\\nTheyearlyseasonalSFEDcompositeswerethenprocessed/reclassifiedintwodistinctways:\\n1. Compositesreclassifiedtobinaryusinga20percentfloodfractionthreshold.\\n2. Composites masked to just remove any flood fraction values < 0.05 percent and the\\nfloodfraction (0-100percent)retained.\\nAll composites in both sets of processed yearly-seasonal flood fraction rasters were then\\nmultiplied by the WorldPop (2020 UNAdjusted)populationestimaterastertocreatetwosetsof\\nyearly seasonal flood exposure rasters. These were then aggregated at the second\\nadministrative level via zonal statistics (sum) to obtain the estimated population exposure per'}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = ingest_file(\"data_raw/sample.pdf\")  # reuse from Step 2\n",
    "chunks = chunk_documents(docs)\n",
    "\n",
    "print(\"Original pages:\", len(docs))\n",
    "print(\"Total chunks:\", len(chunks))\n",
    "chunks[:2]  # show first two chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fcbdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_chunks(chunks, filename=\"processed.json\"):\n",
    "    path = f\"data_processed/{filename}\"\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(chunks, f, indent=2)\n",
    "    print(\"Saved:\", path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57b78b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data_processed/sample_chunks.json\n"
     ]
    }
   ],
   "source": [
    "save_chunks(chunks, \"sample_chunks.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31ff5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chunks(filename=\"processed.json\"):\n",
    "    with open(f\"data_processed/{filename}\", \"r\") as f:\n",
    "        return json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546f7c08",
   "metadata": {},
   "source": [
    "# Embeddings + Vector Database (FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39e2202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded chunks: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'source': 'data_raw/sample.pdf',\n",
       "  'chunk_id': 0,\n",
       "  'text': 'Somalia Flood Exposure Methodology Note\\nAnalysis for 2024 HNRP\\nThis technical note summarises the methodology used to calculate the number of people\\npotentially exposed to flooding in Somalia in the 2024 Somalia Humanitarian Needs and\\nResponse Plan (HNRP). The UN OCHA Centre for Humanitarian Data worked with awiderange\\nof technical partners to develop a methodology that was then endorsed by the Somalia ICCG\\nandHCT.\\nDaily FloodScan (1998-2022) & WorldPop (2020 UN Adjusted) raster data wasanalysedtogain\\nunderstanding of flood conditions across Somalia for both March-April-May (MAM) and\\nOctober-November-December (OND) seasons. FloodScan daily flood fraction Standard Flood\\nExposure Depiction (SFED) was aggregated to yearly seasonal maximum fraction composites'},\n",
       " {'source': 'data_raw/sample.pdf',\n",
       "  'chunk_id': 1,\n",
       "  'text': 'Exposure Depiction (SFED) was aggregated to yearly seasonal maximum fraction composites\\nforboththeMAMandONDseasonsforallyearsofhistoricalFloodScandata(1998-2022).\\nTheyearlyseasonalSFEDcompositeswerethenprocessed/reclassifiedintwodistinctways:\\n1. Compositesreclassifiedtobinaryusinga20percentfloodfractionthreshold.\\n2. Composites masked to just remove any flood fraction values < 0.05 percent and the\\nfloodfraction (0-100percent)retained.\\nAll composites in both sets of processed yearly-seasonal flood fraction rasters were then\\nmultiplied by the WorldPop (2020 UNAdjusted)populationestimaterastertocreatetwosetsof\\nyearly seasonal flood exposure rasters. These were then aggregated at the second\\nadministrative level via zonal statistics (sum) to obtain the estimated population exposure per'}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_chunks(filename):\n",
    "    with open(f\"data_processed/{filename}\", \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "chunks = load_chunks(\"sample_chunks.json\")\n",
    "print(\"Loaded chunks:\", len(chunks))\n",
    "chunks[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd24c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded!\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"Embedding model loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dc3ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5, 384)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def embed_chunks(chunks):\n",
    "    texts = [c[\"text\"] for c in chunks]\n",
    "    embeddings = embed_model.encode(\n",
    "        texts, \n",
    "        convert_to_numpy=True, \n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "embeddings = embed_chunks(chunks)\n",
    "embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64724339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index size: 5\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "def build_faiss_index(embeddings):\n",
    "    dim = embeddings.shape[1]\n",
    "\n",
    "    # Create IndexFlatIP (Inner Product = cosine similarity after normalization)\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "\n",
    "    # Normalize to make inner product = cosine similarity\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    \n",
    "    # Add vectors to index\n",
    "    index.add(embeddings)\n",
    "\n",
    "    return index\n",
    "\n",
    "index = build_faiss_index(embeddings)\n",
    "print(\"FAISS index size:\", index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce4c5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: faiss_store.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def save_faiss(index, chunks, embeddings, filename=\"faiss_store.pkl\"):\n",
    "    store = {\n",
    "        \"index\": index,\n",
    "        \"chunks\": chunks,\n",
    "        \"embeddings\": embeddings\n",
    "    }\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(store, f)\n",
    "    print(\"Saved:\", filename)\n",
    "\n",
    "save_faiss(index, chunks, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a83fd37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['index', 'chunks', 'embeddings'])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_faiss(filename=\"faiss_store.pkl\"):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "faiss_store = load_faiss()\n",
    "faiss_store.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68794f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "Score: 0.13010777533054352\n",
      "Source: data_raw/sample.pdf\n",
      "Chunk: 0\n",
      "Text Preview: Somalia Flood Exposure Methodology Note\n",
      "Analysis for 2024 HNRP\n",
      "This technical note summarises the methodology used to calculate the number of people\n",
      "potentially exposed to flooding in Somalia in the 2024 Somalia Humanitarian Needs and\n",
      "Response Plan (HNRP). The UN OCHA Centre for Humanitarian Data wo ...\n",
      "-----\n",
      "Score: 0.11402948200702667\n",
      "Source: data_raw/sample.pdf\n",
      "Chunk: 3\n",
      "Text Preview: season for MAM 2024, the range of populationexposed wasbasedonthe50th-95thpercentile\n",
      "levels. Since there was no available data to inform predictions for OND 2024, the 25-75th\n",
      "percentile values were used to bound the range. The lower and upper limits of therangeswere\n",
      "Page1of2 ...\n",
      "-----\n",
      "Score: 0.09291001409292221\n",
      "Source: data_raw/sample.pdf\n",
      "Chunk: 0\n",
      "Text Preview: calculated at the administrative level using all of the historical yearly seasonal flood exposure\n",
      "estimates. The two sets of range estimates were combined conservatively by taking the\n",
      "maximumvalueforboththeupperandlowerboundsforeachseason.\n",
      "Contact the OCHA Centre for Humanitarian Data via Leonardo M ...\n"
     ]
    }
   ],
   "source": [
    "def search_index(query, store, k=3):\n",
    "    index = store[\"index\"]\n",
    "    chunks = store[\"chunks\"]\n",
    "\n",
    "    q_emb = embed_model.encode([query], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(q_emb)\n",
    "\n",
    "    scores, indices = index.search(q_emb, k)\n",
    "\n",
    "    results = []\n",
    "    for idx, score in zip(indices[0], scores[0]):\n",
    "        results.append({\n",
    "            \"score\": float(score),\n",
    "            \"source\": chunks[idx][\"source\"],\n",
    "            \"chunk_id\": chunks[idx][\"chunk_id\"],\n",
    "            \"text\": chunks[idx][\"text\"]\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "query = \"What is the document talking about?\"\n",
    "results = search_index(query, faiss_store, k=3)\n",
    "\n",
    "for r in results:\n",
    "    print(\"-----\")\n",
    "    print(\"Score:\", r[\"score\"])\n",
    "    print(\"Source:\", r[\"source\"])\n",
    "    print(\"Chunk:\", r[\"chunk_id\"])\n",
    "    print(\"Text Preview:\", r[\"text\"][:300], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87a50f5",
   "metadata": {},
   "source": [
    "# Reranking + LLM Answer Generation + Citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59da7574",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (5.1.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (4.57.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (1.6.0)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (12.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers\n",
    "!pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb2ceda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranker loaded!\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "print(\"Reranker loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac509ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_results(query, retrieved_chunks):\n",
    "    pairs = [[query, item[\"text\"]] for item in retrieved_chunks]\n",
    "    scores = reranker.predict(pairs)\n",
    "\n",
    "    for item, score in zip(retrieved_chunks, scores):\n",
    "        item[\"rerank_score\"] = float(score)\n",
    "\n",
    "    # Sort by score (descending)\n",
    "    ranked = sorted(retrieved_chunks, key=lambda x: x[\"rerank_score\"], reverse=True)\n",
    "\n",
    "    return ranked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751ed47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, top_chunks):\n",
    "    context = \"\"\n",
    "\n",
    "    for c in top_chunks:\n",
    "        context += f\"\\n[Source: {c['source']} | Chunk: {c['chunk_id']}] \\n{c['text']}\\n\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant. Use ONLY the information from the context below to answer the user's question.\n",
    "\n",
    "If the context does not contain the answer, reply:\n",
    "\"I don’t know based on the provided documents.\"\n",
    "\n",
    "Always include a \"Sources:\" section at the end.\n",
    "\n",
    "### CONTEXT:\n",
    "{context}\n",
    "\n",
    "### QUESTION:\n",
    "{query}\n",
    "\n",
    "### ANSWER (with citations):\n",
    "\"\"\"\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332e7b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROQ KEY = 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\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "print(\"GROQ KEY =\", os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "from groq import Groq\n",
    "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40811d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/apoorvnathtripathi/Desktop/multi_source_rag_project\n",
      "Environment key value: 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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Environment key value:\", os.getenv(\"GROQ_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374f37f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'The model `llama3-70b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama3-70b-8192\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSay hello\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(test\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/new_env/lib/python3.12/site-packages/groq/resources/chat/completions.py:464\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    303\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m not_given,\n\u001b[1;32m    304\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    305\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/openai/v1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcitation_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcitation_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompound_custom\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompound_custom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdisable_tool_validation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_tool_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocuments\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexclude_domains\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minclude_domains\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minclude_reasoning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_reasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msearch_settings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/new_env/lib/python3.12/site-packages/groq/_base_client.py:1242\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1230\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1239\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1240\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1241\u001b[0m     )\n\u001b[0;32m-> 1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/new_env/lib/python3.12/site-packages/groq/_base_client.py:1044\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1041\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1043\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1044\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'The model `llama3-70b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}"
     ]
    }
   ],
   "source": [
    "test = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Say hello\"}]\n",
    ")\n",
    "\n",
    "print(test.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72e3882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<groq.Groq at 0x123856b70>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667a2105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR TYPE: <class 'groq.BadRequestError'>\n",
      "ERROR DETAILS: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    test = client.chat.completions.create(\n",
    "        model=\"llama-3.1-70b-versatile\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Say hello\"}],\n",
    "        max_tokens=20\n",
    "    )\n",
    "    print(test.choices[0].message.content)\n",
    "\n",
    "except Exception as e:\n",
    "    import json\n",
    "    print(\"ERROR TYPE:\", type(e))\n",
    "    try:\n",
    "        print(\"ERROR DETAILS:\", e.args[0])\n",
    "    except:\n",
    "        print(\"RAW ERROR:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533f7893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_groq(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac44c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline_groq(query, store, k=5):\n",
    "    # 1. Retrieve from FAISS\n",
    "    retrieved = search_index(query, store, k)\n",
    "\n",
    "    # 2. Rerank retrieved chunks\n",
    "    ranked = rerank_results(query, retrieved)\n",
    "\n",
    "    # 3. Pick top 3\n",
    "    top_chunks = ranked[:3]\n",
    "\n",
    "    # 4. Build the LLM prompt\n",
    "    prompt = build_prompt(query, top_chunks)\n",
    "    \n",
    "    # 5. Get the answer from Groq LLaMA 3.1\n",
    "    answer = generate_answer_groq(prompt)\n",
    "\n",
    "    return answer, top_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bc8406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groq/compound-mini\n",
      "meta-llama/llama-prompt-guard-2-22m\n",
      "meta-llama/llama-prompt-guard-2-86m\n",
      "playai-tts-arabic\n",
      "openai/gpt-oss-safeguard-20b\n",
      "qwen/qwen3-32b\n",
      "moonshotai/kimi-k2-instruct-0905\n",
      "whisper-large-v3\n",
      "llama-3.3-70b-versatile\n",
      "playai-tts\n",
      "meta-llama/llama-4-scout-17b-16e-instruct\n",
      "whisper-large-v3-turbo\n",
      "groq/compound\n",
      "meta-llama/llama-guard-4-12b\n",
      "moonshotai/kimi-k2-instruct\n",
      "openai/gpt-oss-120b\n",
      "openai/gpt-oss-20b\n",
      "llama-3.1-8b-instant\n",
      "meta-llama/llama-4-maverick-17b-128e-instruct\n",
      "allam-2-7b\n"
     ]
    }
   ],
   "source": [
    "models = client.models.list()\n",
    "for m in models.data:\n",
    "    print(m.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91b4dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:\n",
      " The document is discussing the methodology used to calculate the number of people potentially exposed to flooding in Somalia, specifically for the 2024 Somalia Humanitarian Needs and Response Plan (HNRP). It outlines the process of aggregating historical flood data, reclassifying and processing the data, and estimating population exposure to flooding.\n",
      "\n",
      "Sources:\n",
      "[Source: data_raw/sample.pdf | Chunk: 1], \n",
      "[Source: data_raw/sample.pdf | Chunk: 0]\n",
      "\n",
      "--- SOURCES USED ---\n",
      "data_raw/sample.pdf (chunk 1)\n",
      "data_raw/sample.pdf (chunk 0)\n",
      "data_raw/sample.pdf (chunk 0)\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the document talking about?\"\n",
    "\n",
    "answer, used_chunks = rag_pipeline_groq(query, faiss_store)\n",
    "\n",
    "\n",
    "print(\"ANSWER:\\n\", answer)\n",
    "print(\"\\n--- SOURCES USED ---\")\n",
    "for c in used_chunks:\n",
    "    print(f\"{c['source']} (chunk {c['chunk_id']})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
