{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec9f3054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-1.0.8-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.13.0-cp39-abi3-macosx_14_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting streamlit\n",
      "  Downloading streamlit-1.51.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting python-docx\n",
      "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.8-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.14.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (2.32.3)\n",
      "Collecting langchain-core<2.0.0,>=1.0.6 (from langchain)\n",
      "  Downloading langchain_core-1.0.7-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting langgraph<1.1.0,>=1.0.2 (from langchain)\n",
      "  Using cached langgraph-1.0.3-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Using cached pydantic-2.12.4-py3-none-any.whl.metadata (89 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.9.1-cp312-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (1.6.0)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (1.14.1)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-1.1.4-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (12.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from faiss-cpu) (2.2.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from faiss-cpu) (24.2)\n",
      "Collecting altair!=5.4.0,!=5.4.1,<6,>=4.0 (from streamlit)\n",
      "  Downloading altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (1.9.0)\n",
      "Collecting cachetools<7,>=4.0 (from streamlit)\n",
      "  Downloading cachetools-6.2.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (2.2.3)\n",
      "Collecting protobuf<7,>=3.20 (from streamlit)\n",
      "  Downloading protobuf-6.33.1-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Collecting pyarrow<22,>=7.0 (from streamlit)\n",
      "  Downloading pyarrow-21.0.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting tenacity<10,>=8.1.0 (from streamlit)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Downloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from streamlit) (6.4.2)\n",
      "Collecting lxml>=3.1.0 (from python-docx)\n",
      "  Downloading lxml-6.0.2-cp312-cp312-macosx_10_13_universal2.whl.metadata (3.6 kB)\n",
      "Collecting pdfminer.six==20251107 (from pdfplumber)\n",
      "  Downloading pdfminer_six-20251107-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-5.0.0-py3-none-macosx_11_0_arm64.whl.metadata (67 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pdfminer.six==20251107->pdfplumber) (3.4.0)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six==20251107->pdfplumber)\n",
      "  Downloading cryptography-46.0.3-cp311-abi3-macosx_10_9_universal2.whl.metadata (5.7 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Using cached soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.4)\n",
      "Collecting jsonschema>=3.0 (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting narwhals>=1.14.2 (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit)\n",
      "  Downloading narwhals-2.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pyyaml>=5.1 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading pyyaml-6.0.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
      "Collecting shellingham (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer-slim (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<2.0.0,>=1.0.6->langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core<2.0.0,>=1.0.6->langchain)\n",
      "  Downloading langsmith-0.4.44-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
      "  Using cached langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
      "  Downloading langgraph_prebuilt-1.0.4-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
      "  Using cached langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting xxhash>=3.5.0 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
      "  Downloading xxhash-3.6.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading pydantic_core-2.41.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
      "Collecting typing_extensions>=4.5.0 (from sentence-transformers)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading regex-2025.11.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: cffi>=2.0.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.0.0)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.6->langchain)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit)\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit)\n",
      "  Using cached referencing-0.37.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit)\n",
      "  Downloading rpds_py-0.29.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting ormsgpack>=1.12.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain)\n",
      "  Downloading ormsgpack-1.12.0-cp312-cp312-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl.metadata (1.2 kB)\n",
      "Collecting orjson>=3.10.1 (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain)\n",
      "  Downloading orjson-3.11.4-cp312-cp312-macosx_15_0_arm64.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard>=0.23.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain)\n",
      "  Downloading zstandard-0.25.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.23)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Downloading langchain-1.0.8-py3-none-any.whl (93 kB)\n",
      "Downloading sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
      "Downloading faiss_cpu-1.13.0-cp39-abi3-macosx_14_0_arm64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading streamlit-1.51.0-py3-none-any.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
      "Downloading pdfplumber-0.11.8-py3-none-any.whl (60 kB)\n",
      "Downloading pdfminer_six-20251107-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached beautifulsoup4-4.14.2-py3-none-any.whl (106 kB)\n",
      "Downloading altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.2/731.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-6.2.2-py3-none-any.whl (11 kB)\n",
      "Downloading gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Downloading langchain_core-1.0.7-py3-none-any.whl (472 kB)\n",
      "Using cached langgraph-1.0.3-py3-none-any.whl (156 kB)\n",
      "Downloading lxml-6.0.2-cp312-cp312-macosx_10_13_universal2.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.33.1-cp39-abi3-macosx_10_9_universal2.whl (427 kB)\n",
      "Downloading pyarrow-21.0.0-cp312-cp312-macosx_12_0_arm64.whl (31.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.2/31.2 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pydantic-2.12.4-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp312-cp312-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pypdfium2-5.0.0-py3-none-macosx_11_0_arm64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached soupsieve-2.8-py3-none-any.whl (36 kB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading torch-2.9.1-cp312-none-macosx_11_0_arm64.whl (74.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading cryptography-46.0.3-cp311-abi3-macosx_10_9_universal2.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
      "Using cached langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
      "Downloading langgraph_prebuilt-1.0.4-py3-none-any.whl (34 kB)\n",
      "Using cached langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
      "Downloading langsmith-0.4.44-py3-none-any.whl (410 kB)\n",
      "Downloading narwhals-2.12.0-py3-none-any.whl (425 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyyaml-6.0.3-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "Downloading regex-2025.11.3-cp312-cp312-macosx_11_0_arm64.whl (288 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl (447 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading xxhash-3.6.0-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.11.4-cp312-cp312-macosx_15_0_arm64.whl (128 kB)\n",
      "Downloading ormsgpack-1.12.0-cp312-cp312-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl (369 kB)\n",
      "Using cached referencing-0.37.0-py3-none-any.whl (26 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading rpds_py-0.29.0-cp312-cp312-macosx_11_0_arm64.whl (360 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading zstandard-0.25.0-cp312-cp312-macosx_11_0_arm64.whl (640 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m640.4/640.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: mpmath, zstandard, xxhash, typing_extensions, toml, tenacity, sympy, soupsieve, sniffio, smmap, safetensors, rpds-py, regex, pyyaml, pypdfium2, pyarrow, protobuf, ormsgpack, orjson, networkx, narwhals, lxml, jsonpointer, hf-xet, h11, fsspec, filelock, faiss-cpu, cachetools, attrs, annotated-types, typing-inspection, torch, requests-toolbelt, referencing, python-docx, pydeck, pydantic-core, jsonpatch, huggingface-hub, httpcore, gitdb, cryptography, beautifulsoup4, anyio, tokenizers, pydantic, pdfminer.six, jsonschema-specifications, httpx, gitpython, transformers, pdfplumber, langsmith, langgraph-sdk, jsonschema, sentence-transformers, langchain-core, altair, streamlit, langgraph-checkpoint, langgraph-prebuilt, langgraph, langchain\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "Successfully installed altair-5.5.0 annotated-types-0.7.0 anyio-4.11.0 attrs-25.4.0 beautifulsoup4-4.14.2 cachetools-6.2.2 cryptography-46.0.3 faiss-cpu-1.13.0 filelock-3.20.0 fsspec-2025.10.0 gitdb-4.0.12 gitpython-3.1.45 h11-0.16.0 hf-xet-1.2.0 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-0.36.0 jsonpatch-1.33 jsonpointer-3.0.0 jsonschema-4.25.1 jsonschema-specifications-2025.9.1 langchain-1.0.8 langchain-core-1.0.7 langgraph-1.0.3 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.4 langgraph-sdk-0.2.9 langsmith-0.4.44 lxml-6.0.2 mpmath-1.3.0 narwhals-2.12.0 networkx-3.5 orjson-3.11.4 ormsgpack-1.12.0 pdfminer.six-20251107 pdfplumber-0.11.8 protobuf-6.33.1 pyarrow-21.0.0 pydantic-2.12.4 pydantic-core-2.41.5 pydeck-0.9.1 pypdfium2-5.0.0 python-docx-1.2.0 pyyaml-6.0.3 referencing-0.37.0 regex-2025.11.3 requests-toolbelt-1.0.0 rpds-py-0.29.0 safetensors-0.7.0 sentence-transformers-5.1.2 smmap-5.0.2 sniffio-1.3.1 soupsieve-2.8 streamlit-1.51.0 sympy-1.14.0 tenacity-9.1.2 tokenizers-0.22.1 toml-0.10.2 torch-2.9.1 transformers-4.57.1 typing-inspection-0.4.2 typing_extensions-4.15.0 xxhash-3.6.0 zstandard-0.25.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain sentence-transformers faiss-cpu streamlit python-docx pdfplumber beautifulsoup4 requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37e2881f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/new_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup successful!\n",
      "Embedding shape: (1, 384)\n",
      "Index size: 1\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "print(\"Setup successful!\")\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vec = model.encode([\"hello world\"])\n",
    "print(\"Embedding shape:\", vec.shape)\n",
    "\n",
    "# FAISS test\n",
    "dim = vec.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "faiss.normalize_L2(vec)\n",
    "index.add(vec)\n",
    "print(\"Index size:\", index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41b1d70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders created: ['data_raw', 'data_processed']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folders = [\"data_raw\", \"data_processed\"]\n",
    "for f in folders:\n",
    "    os.makedirs(f, exist_ok=True)\n",
    "\n",
    "print(\"Folders created:\", folders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6c9ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def load_pdf(path):\n",
    "    pages_text = []\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text() or \"\"\n",
    "            pages_text.append(text)\n",
    "    return pages_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fea0dab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Somalia Flood Exposure Methodology Note\\nAnalysis for 2024 HNRP\\nThis technical note summarises the methodology used to calculate the number of people\\npotentially exposed to flooding in Somalia in the 2024 Somalia Humanitarian Needs and\\nResponse Plan (HNRP). The UN OCHA Centre for Humanitarian Data worked with awiderange\\nof technical partners to develop a methodology that was then endorsed by the Somalia ICCG\\nandHCT.\\nDaily FloodScan (1998-2022) & WorldPop (2020 UN Adjusted) raster data wasanalysedtogain\\nunderstanding of flood conditions across Somalia for both March-April-May (MAM) and\\nOctober-November-December (OND) seasons. FloodScan daily flood fraction Standard Flood\\nExposure Depiction (SFED) was aggregated to yearly seasonal maximum fraction composites\\nforboththeMAMandONDseasonsforallyearsofhistoricalFloodScandata(1998-2022).\\nTheyearlyseasonalSFEDcompositeswerethenprocessed/reclassifiedintwodistinctways:\\n1. Compositesreclassifiedtobinaryusinga20percentfloodfractionthreshold.\\n2. Composites masked to just remove any flood fraction values < 0.05 percent and the\\nfloodfraction (0-100percent)retained.\\nAll composites in both sets of processed yearly-seasonal flood fraction rasters were then\\nmultiplied by the WorldPop (2020 UNAdjusted)populationestimaterastertocreatetwosetsof\\nyearly seasonal flood exposure rasters. These were then aggregated at the second\\nadministrative level via zonal statistics (sum) to obtain the estimated population exposure per\\ndistrict for each season across all years for each set. These estimates were converted to the\\npercent of population exposed by dividing the estimates by the total population per district\\n(from World Pop raster). The percent exposure figure was then applied to the updated 2024\\npopulation data set (UN OCHA, UNFPA Methodology) to obtain updated population exposure\\nestimates.Thesedatawerefurtheraggregatedtoadministrativelevel1.\\nRanges for exposure were estimated using percentiles for boththeMAMandONDseasonsper\\nadministrative level. As ECMWF seasonal forecast predicts an above average precipitation\\nseason for MAM 2024, the range of populationexposed wasbasedonthe50th-95thpercentile\\nlevels. Since there was no available data to inform predictions for OND 2024, the 25-75th\\npercentile values were used to bound the range. The lower and upper limits of therangeswere\\nPage1of2']\n"
     ]
    }
   ],
   "source": [
    "pages = load_pdf('data_raw/sample.pdf')\n",
    "print(pages[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3e3a83",
   "metadata": {},
   "source": [
    "# pull text from any website URL so it can use that information for retrieval & answering questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee7ac0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Domain\n",
      "Example Domain\n",
      "This domain is for use in documentation examples without needing permission. Avoid use in operations.\n",
      "Learn more\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def load_url(url):\n",
    "    response = requests.get(url, timeout=10)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # remove script + style\n",
    "    for tag in soup([\"script\", \"style\"]):\n",
    "        tag.decompose()\n",
    "    \n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "    clean_text = \"\\n\".join([line.strip() for line in text.splitlines() if line.strip()])\n",
    "    return clean_text\n",
    "\n",
    "# Example:\n",
    "text = load_url(\"https://example.com\")\n",
    "print(text[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "400ab32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_file(path_or_url):\n",
    "    if path_or_url.startswith(\"http\"):\n",
    "        content = load_url(path_or_url)\n",
    "        return [{\"source\": path_or_url, \"text\": content}]\n",
    "    \n",
    "    if path_or_url.endswith(\".pdf\"):\n",
    "        pages = load_pdf(path_or_url)\n",
    "        return [{\"source\": path_or_url, \"text\": p} for p in pages]\n",
    "    \n",
    "    if path_or_url.endswith(\".csv\"):\n",
    "        rows = load_csv(path_or_url)\n",
    "        return [{\"source\": path_or_url, \"text\": r} for r in rows]\n",
    "    \n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de123f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 2 items\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'source': 'data_raw/sample.pdf',\n",
       "  'text': 'Somalia Flood Exposure Methodology Note\\nAnalysis for 2024 HNRP\\nThis technical note summarises the methodology used to calculate the number of people\\npotentially exposed to flooding in Somalia in the 2024 Somalia Humanitarian Needs and\\nResponse Plan (HNRP). The UN OCHA Centre for Humanitarian Data worked with awiderange\\nof technical partners to develop a methodology that was then endorsed by the Somalia ICCG\\nandHCT.\\nDaily FloodScan (1998-2022) & WorldPop (2020 UN Adjusted) raster data wasanalysedtogain\\nunderstanding of flood conditions across Somalia for both March-April-May (MAM) and\\nOctober-November-December (OND) seasons. FloodScan daily flood fraction Standard Flood\\nExposure Depiction (SFED) was aggregated to yearly seasonal maximum fraction composites\\nforboththeMAMandONDseasonsforallyearsofhistoricalFloodScandata(1998-2022).\\nTheyearlyseasonalSFEDcompositeswerethenprocessed/reclassifiedintwodistinctways:\\n1. Compositesreclassifiedtobinaryusinga20percentfloodfractionthreshold.\\n2. Composites masked to just remove any flood fraction values < 0.05 percent and the\\nfloodfraction (0-100percent)retained.\\nAll composites in both sets of processed yearly-seasonal flood fraction rasters were then\\nmultiplied by the WorldPop (2020 UNAdjusted)populationestimaterastertocreatetwosetsof\\nyearly seasonal flood exposure rasters. These were then aggregated at the second\\nadministrative level via zonal statistics (sum) to obtain the estimated population exposure per\\ndistrict for each season across all years for each set. These estimates were converted to the\\npercent of population exposed by dividing the estimates by the total population per district\\n(from World Pop raster). The percent exposure figure was then applied to the updated 2024\\npopulation data set (UN OCHA, UNFPA Methodology) to obtain updated population exposure\\nestimates.Thesedatawerefurtheraggregatedtoadministrativelevel1.\\nRanges for exposure were estimated using percentiles for boththeMAMandONDseasonsper\\nadministrative level. As ECMWF seasonal forecast predicts an above average precipitation\\nseason for MAM 2024, the range of populationexposed wasbasedonthe50th-95thpercentile\\nlevels. Since there was no available data to inform predictions for OND 2024, the 25-75th\\npercentile values were used to bound the range. The lower and upper limits of therangeswere\\nPage1of2'},\n",
       " {'source': 'data_raw/sample.pdf',\n",
       "  'text': 'calculated at the administrative level using all of the historical yearly seasonal flood exposure\\nestimates. The two sets of range estimates were combined conservatively by taking the\\nmaximumvalueforboththeupperandlowerboundsforeachseason.\\nContact the OCHA Centre for Humanitarian Data via Leonardo Milano, Team Lead for Data\\nScienceatleonardo.milano@un.orgwithanyquestionsorfeedback.\\nPage2of2'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pdf = \"data_raw/sample.pdf\"\n",
    "docs = ingest_file(test_pdf)\n",
    "\n",
    "print(\"Loaded:\", len(docs), \"items\")\n",
    "docs[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d58a914",
   "metadata": {},
   "source": [
    "# Processing & Chunking Text\n",
    "Chunking is critical for RAG.\n",
    "Instead of sending entire documents to the model, we break them into small meaningful pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "588b5720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (1.0.8)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.6 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain) (1.0.7)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain) (1.0.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain) (2.12.4)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (0.4.44)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (24.2)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (4.15.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.4)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.9)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.6->langchain) (3.0.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (2.32.3)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (0.25.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (4.11.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (2.2.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-text-splitters\n",
      "  Downloading langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-text-splitters) (1.0.7)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.4.44)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.12.4)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.32.3)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.4.2)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (4.11.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.2.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/new_env/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.3.1)\n",
      "Downloading langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: langchain-text-splitters\n",
      "Successfully installed langchain-text-splitters-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain-text-splitters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75846c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2b77629",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=150,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "def chunk_documents(docs):\n",
    "    \"\"\"\n",
    "    Input: docs = [{\"source\": \"...\", \"text\": \"...\"}]\n",
    "    Output: list of chunks with metadata\n",
    "    \"\"\"\n",
    "    processed = []\n",
    "\n",
    "    for d in docs:\n",
    "        chunks = text_splitter.split_text(d[\"text\"])\n",
    "        for i, c in enumerate(chunks):\n",
    "            processed.append({\n",
    "                \"source\": d[\"source\"],\n",
    "                \"chunk_id\": i,\n",
    "                \"text\": c\n",
    "            })\n",
    "\n",
    "    return processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eccbdda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original pages: 2\n",
      "Total chunks: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'source': 'data_raw/sample.pdf',\n",
       "  'chunk_id': 0,\n",
       "  'text': 'Somalia Flood Exposure Methodology Note\\nAnalysis for 2024 HNRP\\nThis technical note summarises the methodology used to calculate the number of people\\npotentially exposed to flooding in Somalia in the 2024 Somalia Humanitarian Needs and\\nResponse Plan (HNRP). The UN OCHA Centre for Humanitarian Data worked with awiderange\\nof technical partners to develop a methodology that was then endorsed by the Somalia ICCG\\nandHCT.\\nDaily FloodScan (1998-2022) & WorldPop (2020 UN Adjusted) raster data wasanalysedtogain\\nunderstanding of flood conditions across Somalia for both March-April-May (MAM) and\\nOctober-November-December (OND) seasons. FloodScan daily flood fraction Standard Flood\\nExposure Depiction (SFED) was aggregated to yearly seasonal maximum fraction composites'},\n",
       " {'source': 'data_raw/sample.pdf',\n",
       "  'chunk_id': 1,\n",
       "  'text': 'Exposure Depiction (SFED) was aggregated to yearly seasonal maximum fraction composites\\nforboththeMAMandONDseasonsforallyearsofhistoricalFloodScandata(1998-2022).\\nTheyearlyseasonalSFEDcompositeswerethenprocessed/reclassifiedintwodistinctways:\\n1. Compositesreclassifiedtobinaryusinga20percentfloodfractionthreshold.\\n2. Composites masked to just remove any flood fraction values < 0.05 percent and the\\nfloodfraction (0-100percent)retained.\\nAll composites in both sets of processed yearly-seasonal flood fraction rasters were then\\nmultiplied by the WorldPop (2020 UNAdjusted)populationestimaterastertocreatetwosetsof\\nyearly seasonal flood exposure rasters. These were then aggregated at the second\\nadministrative level via zonal statistics (sum) to obtain the estimated population exposure per'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = ingest_file(\"data_raw/sample.pdf\")  # reuse from Step 2\n",
    "chunks = chunk_documents(docs)\n",
    "\n",
    "print(\"Original pages:\", len(docs))\n",
    "print(\"Total chunks:\", len(chunks))\n",
    "chunks[:2]  # show first two chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04fcbdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_chunks(chunks, filename=\"processed.json\"):\n",
    "    path = f\"data_processed/{filename}\"\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(chunks, f, indent=2)\n",
    "    print(\"Saved:\", path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a57b78b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data_processed/sample_chunks.json\n"
     ]
    }
   ],
   "source": [
    "save_chunks(chunks, \"sample_chunks.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c31ff5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chunks(filename=\"processed.json\"):\n",
    "    with open(f\"data_processed/{filename}\", \"r\") as f:\n",
    "        return json.load(f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
